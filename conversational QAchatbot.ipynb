{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60629dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002368BEBFC80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002368BF34320>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()   \n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")  # âœ… Correct spelling\n",
    "\n",
    "model= ChatGroq(\n",
    "    model_name=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c32741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings( model=\"mxbai-embed-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fcfa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "loader=WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\",\"post-content-inner\",\"post-content-inner-wrapper\",\"post-title\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9135df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa647d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000236857BFDD0>, search_kwargs={})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1270b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=(\"you are a helpful assistant that answers questions based on the provided context. If the context does not contain the answer, you should say 'I don't know'.use three sentences to answer the question. If the context is not enough, you should say 'I don't know' and ask the user to provide more information.\"\"\\n\\n\"\"{context}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",system_prompt), (\"human\", \"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa1777ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e50bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflection, in the context of this document, is the process of an agent analyzing past actions and identifying areas for improvement. \n",
      "\n",
      "This analysis is used to guide future decision-making and correct past mistakes. \n",
      "\n",
      "It's achieved by showing the agent examples of failed trajectories paired with ideal reflections that suggest changes to the plan. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is the self reflection?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05f1870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding the chat history to the chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and a question, provide a context for the question based on the chat history. \"\n",
    "    \"If the chat history does not contain enough information, say 'I don't know' and ask the user to provide more information.\\n\\n\"\n",
    "    \"Chat History:\\n{chat_history}\\n\\nQuestion: {input}\\n\\nContext:\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4287a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f9ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000236857BFDD0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000236E9697420>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'input'], input_types={}, partial_variables={}, template=\"Given a chat history and a question, provide a context for the question based on the chat history. If the chat history does not contain enough information, say 'I don't know' and ask the user to provide more information.\\n\\nChat History:\\n{chat_history}\\n\\nQuestion: {input}\\n\\nContext:\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002368BEBFC80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002368BF34320>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000236857BFDD0>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever= create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbaf3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model,qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b796004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using self-reflection is beneficial because it enables autonomous agents to learn and improve iteratively. \n",
      "\n",
      "By analyzing past actions and identifying failures, agents can generate insights that lead to better decision-making in the future.  \n",
      "\n",
      "This ability to learn from experience is crucial for agents operating in complex and dynamic environments where trial and error are inevitable. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "question= \"What is the self reflection?\"\n",
    "response1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response1[\"answer\"])\n",
    "])\n",
    "\n",
    "question2=\"Tell me more about it?\"\n",
    "response2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question2),\n",
    "    AIMessage(content=response2[\"answer\"])])\n",
    "# print(response2[\"answer\"])\n",
    "\n",
    "question3=\"Why using that?\"\n",
    "response3 = rag_chain.invoke({\"input\": question3, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question3),\n",
    "    AIMessage(content=response3[\"answer\"])])\n",
    "print(response3[\"answer\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
